#!/usr/bin/env bash
# AgentBox - Simplified Docker environment for Claude development
# Automatically rebuilds when Dockerfile changes and uses ephemeral containers with persistent caches

set -euo pipefail

if ((BASH_VERSINFO[0] < 4)); then
    echo "Error: Bash 4.0 or later required (found ${BASH_VERSION}). This project requires 4.0+ to simplify the code." >&2
    if [[ $OSTYPE == darwin* ]]; then
        echo "Due to GPL licensing, macOS ships with Bash 3.2, which is archaic." >&2
        echo "Try: brew install bash, then ensure /opt/homebrew/bin is in your PATH" >&2
    fi
    exit 1
fi

# Configuration
# Resolve symlinks to get the real script location (allows installation as symlink)
readonly SCRIPT_PATH="$(readlink -f "${BASH_SOURCE[0]}")"
readonly SCRIPT_DIR="$(cd "$(dirname "$SCRIPT_PATH")" && pwd)"
readonly PROJECT_DIR="$(pwd)"
readonly PROJECT_NAME="$(basename "$PROJECT_DIR")"
readonly DOCKERFILE_PATH="${SCRIPT_DIR}/Dockerfile"
readonly ENTRYPOINT_PATH="${SCRIPT_DIR}/entrypoint.sh"
readonly IMAGE_NAME="agentbox:latest"
readonly CONTAINER_PREFIX="agentbox"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly CYAN='\033[0;36m'
readonly NC='\033[0m' # No Color

# Helper functions
log_info() { echo -e "${BLUE}â„¹${NC}  $1"; }
log_success() { echo -e "${GREEN}âœ…${NC} $1"; }
log_warning() { echo -e "${YELLOW}âš ${NC}  $1"; }
log_error() { echo -e "${RED}âŒ${NC} $1" >&2; }
log_build() { echo -e "${CYAN}ðŸ”¨${NC} $1"; }

# Check if Docker is installed and running
check_docker() {
    if ! command -v docker &> /dev/null; then
        log_error "Docker is not installed. Please install Docker first."
        exit 1
    fi

    if ! docker info &> /dev/null; then
        log_error "Docker daemon is not running. Please start Docker."
        exit 1
    fi
}

# Calculate hash for a file
calculate_hash() {
    local file="$1"
    if [[ -f "$file" ]]; then
        sha256sum "$file" | cut -d' ' -f1
    else
        echo "none"
    fi
}

# Get container name for current project
# INSTANCE_NAME is always set (auto-assigned or user-specified)
get_container_name() {
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    echo "${CONTAINER_PREFIX}-${project_hash}-${INSTANCE_NAME}"
}

# Get next available instance number for auto-assignment
get_next_instance_number() {
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    local pattern="${CONTAINER_PREFIX}-${project_hash}-"

    # Get all running instance numbers (numeric only)
    local used_numbers=$(docker ps --filter "name=${pattern}" --format "{{.Names}}" 2>/dev/null | \
        sed -n "s/^${pattern}\([0-9][0-9]*\)$/\1/p" | sort -n)

    # Find first unused number starting from 1
    local num=1
    while echo "$used_numbers" | grep -q "^${num}$"; do
        ((num++))
    done

    echo "$num"
}

# List all running instances for current project
list_instances() {
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    local pattern="${CONTAINER_PREFIX}-${project_hash}"

    log_info "Running instances for: $PROJECT_NAME"
    echo ""

    local instances=$(docker ps --filter "name=${pattern}" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" 2>/dev/null)

    if [[ -z "$instances" ]] || [[ $(echo "$instances" | wc -l) -le 1 ]]; then
        echo "  No running instances found."
    else
        echo "$instances"
    fi
    echo ""

    # Also show stopped instances
    local stopped=$(docker ps -a --filter "name=${pattern}" --filter "status=exited" --format "table {{.Names}}\t{{.Status}}" 2>/dev/null)
    if [[ -n "$stopped" ]] && [[ $(echo "$stopped" | wc -l) -gt 1 ]]; then
        log_info "Stopped instances:"
        echo "$stopped"
    fi
}

# Stop a specific instance
stop_instance() {
    local instance_name="$1"
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    local container_name

    if [[ -n "$instance_name" ]]; then
        container_name="${CONTAINER_PREFIX}-${project_hash}-${instance_name}"
    else
        container_name="${CONTAINER_PREFIX}-${project_hash}"
    fi

    if docker ps -q --filter "name=^${container_name}$" | grep -q .; then
        log_info "Stopping instance: $container_name"
        docker stop "$container_name"
        log_success "Instance stopped"
    else
        log_error "Instance not found or not running: $container_name"
        exit 1
    fi
}

# Attach to an existing instance
attach_instance() {
    local instance_name="$1"
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    local container_name

    if [[ -n "$instance_name" ]]; then
        container_name="${CONTAINER_PREFIX}-${project_hash}-${instance_name}"
    else
        container_name="${CONTAINER_PREFIX}-${project_hash}"
    fi

    if docker ps -q --filter "name=^${container_name}$" | grep -q .; then
        log_info "Attaching to instance: $container_name"
        docker attach "$container_name"
    else
        log_error "Instance not found or not running: $container_name"
        log_info "Use 'agentbox list' to see available instances"
        exit 1
    fi
}

# Execute command in an existing instance
exec_instance() {
    local instance_name="$1"
    shift
    local cmd_args=("$@")
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    local container_name

    if [[ -n "$instance_name" ]]; then
        container_name="${CONTAINER_PREFIX}-${project_hash}-${instance_name}"
    else
        container_name="${CONTAINER_PREFIX}-${project_hash}"
    fi

    if docker ps -q --filter "name=^${container_name}$" | grep -q .; then
        if [[ ${#cmd_args[@]} -eq 0 ]]; then
            docker exec -it "$container_name" /bin/zsh
        else
            docker exec -it "$container_name" "${cmd_args[@]}"
        fi
    else
        log_error "Instance not found or not running: $container_name"
        log_info "Use 'agentbox list' to see available instances"
        exit 1
    fi
}

# Get Claude volume name for current project (shared across all instances)
get_claude_volume_name() {
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    echo "agentbox-claude-${project_hash}"
}

# Check if volume exists
check_volume_exists() {
    local volume_name="$1"
    docker volume inspect "$volume_name" &>/dev/null
}


# Check if image needs rebuild
needs_rebuild() {
    # Calculate current Dockerfile and entrypoint hashes
    local dockerfile_hash=$(calculate_hash "$DOCKERFILE_PATH")
    local entrypoint_hash=$(calculate_hash "$ENTRYPOINT_PATH")
    local combined_hash="${dockerfile_hash}-${entrypoint_hash}"

    # Check if image exists
    if ! docker image inspect "$IMAGE_NAME" &> /dev/null; then
        return 0  # Needs rebuild - image doesn't exist
    fi

    # Get stored hash from image
    local stored_hash=$(docker inspect "$IMAGE_NAME" \
        --format '{{ index .Config.Labels "agentbox.hash" }}' 2>/dev/null || echo "none")

    # Compare hashes
    if [[ "$combined_hash" != "$stored_hash" ]]; then
        return 0  # Needs rebuild
    fi

    return 1  # No rebuild needed
}

# Build Docker image
build_image() {
    local dockerfile_hash=$(calculate_hash "$DOCKERFILE_PATH")
    local entrypoint_hash=$(calculate_hash "$ENTRYPOINT_PATH")
    local combined_hash="${dockerfile_hash}-${entrypoint_hash}"

    log_build "Building AgentBox image (this may take a few minutes on first run)..."

    # Get current user/group IDs
    local user_id=$(id -u)
    local group_id=$(id -g)

    # Build with progress output
    local build_timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
    if docker build \
        --build-arg USER_ID="${user_id}" \
        --build-arg GROUP_ID="${group_id}" \
        --build-arg USERNAME="claude" \
        --build-arg BUILD_TIMESTAMP=${build_timestamp} \
        --label "agentbox.hash=${combined_hash}" \
        --label "agentbox.version=1.0.0" \
        --label "agentbox.built=${build_timestamp}" \
        -t "$IMAGE_NAME" \
        "$SCRIPT_DIR" ; then
        log_success "Image built successfully!"
        docker image prune -f --filter "label=agentbox.version" &>/dev/null || true
        return 0
    else
        log_error "Failed to build image"
        return 1
    fi
}

expand_tilde() {
    local path="$1"
    if [[ "$path" =~ ^~ ]]; then
        echo "${path/#\~/$HOME}"
    else
        echo "$path"
    fi
}

has_path_traversal() {
    local path="$1"
    [[ "$path" =~ \.\. ]]
}

is_absolute_path() {
    local path="$1"
    [[ "$path" =~ ^/ ]]
}

is_critical_system_dir() {
    local path="$1"
    [[ "$path" =~ ^/(bin|sbin|boot|lib|lib64)(/|$) ]]
}

is_system_dir() {
    local path="$1"
    [[ "$path" =~ ^/(etc|var|usr|sys|proc|dev)(/|$) ]]
}

is_duplicate_dir() {
    local path="$1"
    local -n check_dirs_ref=$2

    for existing_dir in "${check_dirs_ref[@]}"; do
        if [[ "$path" == "$existing_dir" ]]; then
            return 0
        fi
    done
    return 1
}

validate_dir_path() {
    local dir="$1"
    local -n validated_dirs_ref=$2
    local project_realpath="$3"

    dir=$(expand_tilde "$dir")

    if has_path_traversal "$dir"; then
        log_error "Path traversal not allowed: $dir"
        return 1
    fi

    if ! is_absolute_path "$dir"; then
        log_error "Directory must be absolute path: $dir"
        return 1
    fi

    if [[ ! -d "$dir" ]]; then
        log_error "Directory not found: $dir"
        return 1
    fi

    local dir_realpath
    dir_realpath=$(realpath "$dir" 2>/dev/null) || {
        log_error "Cannot resolve path: $dir"
        return 1
    }

    if is_critical_system_dir "$dir_realpath"; then
        log_error "Mounting critical system directory not allowed: $dir_realpath"
        return 1
    fi

    if [[ "$dir_realpath" == "$project_realpath" ]]; then
        log_warning "Skipping duplicate: $dir (already mounted as project directory)"
        return 0  # Not an error, just skip
    fi

    if is_duplicate_dir "$dir_realpath" validated_dirs_ref; then
        log_warning "Skipping duplicate directory: $dir"
        return 0  # Not an error, just skip
    fi

    if is_system_dir "$dir_realpath"; then
        log_warning "Mounting system directory: $dir_realpath (proceed with caution)"
    fi

    validated_dirs_ref+=("$dir_realpath")
    return 0
}

# Mount additional dirs to Docker mount options
mount_additional_dirs() {
    local -n mount_opts_ref=$1
    shift
    local extra_dirs=("$@")

    # Return early if no dirs to mount
    if [[ ${#extra_dirs[@]} -eq 0 ]]; then
        return 0
    fi

    for extra_dir in "${extra_dirs[@]}"; do
        local dir_name=$(basename "$extra_dir")
        mount_opts_ref+=(-v "$extra_dir:/${dir_name}:z")
        log_info "Mounting additional directory: $extra_dir -> /${dir_name}"
    done
}

# Run container with --rm (ephemeral)
run_container() {
    local container_name="$1"
    local -n extra_dirs_to_mount=$2
    shift 2

    # Check if first arg is "shell" mode
    local shell_mode=false
    local admin_mode=false
    if [[ "${1:-}" == "shell" ]]; then
        shell_mode=true
        shift
        # Check if next arg is --admin
        if [[ "${1:-}" == "--admin" ]]; then
            admin_mode=true
            shift
        fi
    fi

    local cmd_args=("$@")

    if [[ "$admin_mode" == "true" ]]; then
        log_success "Starting container for: $PROJECT_NAME (admin shell)"
    else
        log_success "Starting container for: $PROJECT_NAME"
    fi

    # Prepare mount options
    # Mount project directory using project name for correct project name detection
    local mount_opts=(
        -v "$PROJECT_DIR:/$PROJECT_NAME:z"
    )

    mount_additional_dirs mount_opts "${extra_dirs_to_mount[@]}"

    # Mount .gitconfig to temp location for copying (if it exists)
    if [[ -f "${HOME}/.gitconfig" ]]; then
        mount_opts+=(-v "${HOME}/.gitconfig:/tmp/host_gitconfig:ro")
    fi

    # Mount dedicated AgentBox SSH directory
    local agentbox_ssh="${HOME}/.agentbox/ssh"
    if [[ -d "${agentbox_ssh}" ]]; then
        mount_opts+=(-v "${agentbox_ssh}:/home/claude/.ssh:rw")
        log_info "AgentBox SSH directory mounted (read-write)"
    else
        log_warning "SSH not configured. Run 'agentbox ssh-init' to enable SSH operations."
    fi

    # Mount cache directories for package managers (shared per project)
    local project_hash=$(echo -n "$PROJECT_DIR" | sha256sum | cut -c1-12)
    local cache_dir="${HOME}/.cache/agentbox/${project_hash}"
    mkdir -p "${cache_dir}/npm" "${cache_dir}/pip"

    mount_opts+=(
        -v "${cache_dir}/npm:/home/claude/.npm"
        -v "${cache_dir}/pip:/home/claude/.cache/pip"
    )

    # Mount per-project persistent data (shared per project)
    local project_data_dir="${HOME}/.agentbox/projects/${project_hash}"
    local history_dir="${project_data_dir}/history"
    mkdir -p "${history_dir}"

    # Mount history directory (not individual files to avoid file locking issues)
    mount_opts+=(
        -v "${history_dir}:/home/claude/.shell_history"
        --env "HISTFILE=/home/claude/.shell_history/zsh_history"
    )

    # Use Docker named volume for Claude runtime data (shared per project)
    local claude_volume_name=$(get_claude_volume_name)

    # Create volume if it doesn't exist (for Claude's runtime data like projects/, etc.)
    if ! docker volume inspect "$claude_volume_name" &>/dev/null; then
        log_info "Creating Claude CLI volume for runtime data"
        docker volume create "$claude_volume_name" &>/dev/null
    fi

    mount_opts+=(-v "${claude_volume_name}:/home/claude/.claude")

    # Bind mount all global config files/directories for real-time sync
    local mounted_configs=()
    for config_item in "${CLAUDE_CONFIG_FILES[@]}"; do
        local host_path="${HOME}/.claude/${config_item}"
        local container_path="/home/claude/.claude/${config_item}"

        if [[ -e "$host_path" ]]; then
            mount_opts+=(-v "${host_path}:${container_path}:rw")
            mounted_configs+=("$config_item")
        fi
    done

    if [[ ${#mounted_configs[@]} -gt 0 ]]; then
        log_info "Global config mounted: ${mounted_configs[*]}"
    else
        log_warning "No global config files found in ~/.claude/"
    fi

    # Set Claude config directory environment variable
    mount_opts+=(--env "CLAUDE_CONFIG_DIR=/home/claude/.claude")

    # Pass ANTHROPIC environment variables from host if set
    # if [[ -n "${ANTHROPIC_BASE_URL:-}" ]]; then
    #     mount_opts+=(--env "ANTHROPIC_BASE_URL=${ANTHROPIC_BASE_URL}")
    #     log_info "ANTHROPIC_BASE_URL passed from host"
    # fi
    # if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
    #     mount_opts+=(--env "ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}")
    #     log_info "ANTHROPIC_API_KEY passed from host"
    # fi

    # Network optimization environment variables for Node.js and HTTP clients
    mount_opts+=(
        --env "NODE_OPTIONS=--max-http-header-size=80000"
        --env "UV_THREADPOOL_SIZE=128"
        --env "HTTP_TIMEOUT=300000"
        --env "HTTPS_TIMEOUT=300000"
    )

    # Mount project directory path info for direnv approval translation
    if [[ -f "$PROJECT_DIR/.envrc" ]]; then
        # Pass the host project directory path as an environment variable
        mount_opts+=(--env "HOST_PROJECT_DIR=$PROJECT_DIR")

        # Mount host direnv approvals to check if this .envrc is already approved
        local host_direnv_allow="${HOME}/.local/share/direnv/allow"
        if [[ -d "$host_direnv_allow" ]]; then
            mount_opts+=(-v "${host_direnv_allow}:/tmp/host_direnv_allow:ro")
            log_info "Host direnv approvals will be translated to container"
        fi
    fi

    # Prepare the command to run
    local container_cmd
    if [[ "$shell_mode" == "true" ]]; then
        if [[ "$admin_mode" == "true" ]]; then
            container_cmd=(bash -c "echo 'ðŸ”’ Admin shell - sudo access enabled' && exec ${cmd_args[*]:-/bin/zsh}")
        else
            container_cmd=("${cmd_args[@]:-/bin/zsh}")
        fi
    else
        # Run claude through zsh to get proper environment
        # Always include --dangerously-skip-permissions, append any additional flags
        local claude_cmd="claude --dangerously-skip-permissions"
        if [[ ${#cmd_args[@]} -gt 0 ]]; then
            claude_cmd="$claude_cmd ${cmd_args[*]}"
        fi
        container_cmd=(zsh -c "source ~/.zshrc && exec $claude_cmd")
    fi

    # Check for .env file in project directory and add to docker args
    local env_file_args=()
    if [[ -f "$PROJECT_DIR/.env" ]]; then
        env_file_args+=(--env-file "$PROJECT_DIR/.env")
        log_info ".env file found and will be loaded into container"
    fi

    # Build port forwarding arguments
    local port_args=()
    for port in "${ports[@]}"; do
        # Support both "3002" and "3002:8080" syntax
        if [[ "$port" =~ ^[0-9]+$ ]]; then
            # Simple port number - map to same port
            port_args+=(-p "${port}:${port}")
            log_info "Port forwarding: ${port} -> ${port}"
        elif [[ "$port" =~ ^[0-9]+:[0-9]+$ ]]; then
            # host:container syntax
            port_args+=(-p "${port}")
            log_info "Port forwarding: ${port}"
        else
            log_warning "Invalid port format: ${port} (use '3002' or '3002:8080')"
        fi
    done

    # Run ephemeral container with --rm and network optimizations
    docker run -it --rm \
        --name "$container_name" \
        --hostname "agentbox-$PROJECT_NAME" \
        "${port_args[@]}" \
        "${mount_opts[@]}" \
        "${env_file_args[@]}" \
        -w "/$PROJECT_NAME" \
        --init \
        --shm-size=512m \
        --add-host host.docker.internal:host-gateway \
        --sysctl net.ipv4.tcp_keepalive_time=60 \
        --sysctl net.ipv4.tcp_keepalive_intvl=10 \
        --sysctl net.ipv4.tcp_keepalive_probes=6 \
        --dns 223.5.5.5 \
        --dns 119.29.29.29 \
        "$IMAGE_NAME" \
        "${container_cmd[@]}"
}

# Show help
show_help() {
    cat << EOF
AgentBox - Simplified Docker environment for Claude development

Usage:
    agentbox [OPTIONS] [COMMAND]

Options:
    -h, --help              Show this help message
    -i, --instance NAME     Specify instance name (allows multiple instances per project)
    -p PORT                 Forward a port from host to container (can be used multiple times)
    --cleanup               Remove AgentBox image and cached data
    --rebuild               Force rebuild of the image
    --add-dir DIR           Mount additional directory (can be used multiple times)

Commands:
    shell [--admin]         Start interactive shell instead of Claude CLI
    list                    List all running instances for current project
    stop [NAME]             Stop a running instance (default instance if no name given)
    attach [NAME]           Attach to a running instance
    exec [NAME] [CMD...]    Execute command in a running instance
    ssh-init                Initialize SSH directory for AgentBox
    sync-claude export      Export volume contents to local directory

    If no command is provided, Claude CLI will be started automatically.
    Use 'shell' command to get a shell instead of Claude CLI.
    Use 'shell --admin' to get a shell with sudo privileges.
    Other commands will be executed inside the container.

Multi-Instance Usage:
    agentbox -i dev                 # Start instance named "dev"
    agentbox -i test -p 3002        # Start instance "test" with port 3002
    agentbox list                   # List all instances for this project
    agentbox stop dev               # Stop instance "dev"
    agentbox attach dev             # Attach to instance "dev"
    agentbox exec dev npm test      # Run command in instance "dev"

Examples:
    agentbox                            # Start Claude CLI for current project
    agentbox -i agent1                  # Start named instance "agent1"
    agentbox -i agent2 -p 3002          # Start "agent2" with port 3002
    agentbox -p 3002                    # Forward port 3002
    agentbox -p 3002:8080               # Map host port 3002 to container port 8080
    agentbox shell                      # Start interactive shell instead of Claude
    agentbox shell --admin              # Start admin shell with sudo access
    agentbox --add-dir ~/1 --add-dir ~/2  # Add dirs with Claude CLI
    agentbox python script.py           # Run Python script in container
    agentbox --cleanup                  # Remove image and optionally cached data
    agentbox --rebuild                  # Force rebuild image
    agentbox ssh-init                   # Set up SSH for AgentBox
    agentbox sync-claude export         # Export volume to ./.claude-volume/

Containers are ephemeral and automatically removed when you exit.
Package caches and shell history persist between sessions.
Global config files (~/.claude/settings.json, commands/, etc.) are bind-mounted.

All directories are mounted using their folder names (e.g., host ~/my-app -> container /my-app).
Additional directories are mounted as their basename (e.g., /foo, /bar).
EOF
}

# Remove AgentBox images and cached data
cleanup_all() {
    log_warning "Removing AgentBox images and cached data..."

    # Remove AgentBox image
    if docker image inspect "$IMAGE_NAME" &> /dev/null; then
        log_info "Removing AgentBox image"
        docker rmi "$IMAGE_NAME" &> /dev/null || true
    fi

    # Remove cached data (optional - ask user)
    local cache_base="${HOME}/.cache/agentbox"
    local project_base="${HOME}/.agentbox/projects"

    if [[ -d "$cache_base" ]] || [[ -d "$project_base" ]]; then
        log_warning "This will also remove cached packages and shell history."
        echo -n "Remove cached data? (y/N): "
        read -r response
        if [[ "$response" =~ ^[Yy]$ ]]; then
            [[ -d "$cache_base" ]] && rm -rf "$cache_base"
            [[ -d "$project_base" ]] && rm -rf "$project_base"
            log_success "Cached data removed"
        else
            log_info "Cached data preserved"
        fi
    fi

    log_success "Cleanup complete"
}

# Config files/directories that should be bind mounted from host
readonly CLAUDE_CONFIG_FILES=(
    "settings.json"
    "settings.local.json"
    ".claude.json"
    ".claude.json.backup"
    "commands"
    "ide"
    "plugins"
    "skills"
)

# Pull all files from volume
pull_from_volume() {
    local volume_name="$1"
    local target_dir="$2"

    # Default target directory
    if [[ -z "$target_dir" ]]; then
        target_dir="${PROJECT_DIR}/.claude-volume"
    fi

    # Expand tilde
    target_dir=$(expand_tilde "$target_dir")

    # Create target directory
    mkdir -p "$target_dir"

    log_info "Exporting from volume to: $target_dir"

    docker run --rm \
        -v "${volume_name}:/source:ro" \
        -v "$target_dir:/target" \
        "$IMAGE_NAME" \
        bash -c 'rsync -av /source/ /target/'

    if [[ $? -eq 0 ]]; then
        log_success "All files exported successfully"
        echo ""
        echo "Files saved to: $target_dir"
        ls -la "$target_dir" | head -20
    else
        log_error "Failed to export files"
        return 1
    fi
}

# Show sync-claude help
show_sync_claude_help() {
    cat << EOF
Usage: agentbox sync-claude <command> [options]

Commands:
    export [--to DIR]   Export volume contents to local directory
                        Default: ./.claude-volume/

Examples:
    agentbox sync-claude export                 # Export to ./.claude-volume/
    agentbox sync-claude export --to ~/backup   # Export to specific directory

Note: Global config files (settings.json, commands/, etc.) are now
automatically bind-mounted from ~/.claude/ - no manual sync needed.
EOF
}

# Main sync-claude command
sync_claude_config() {
    local subcommand="${1:-}"
    shift || true

    # Check Docker
    check_docker

    # Get volume name
    local volume_name=$(get_claude_volume_name)

    # Check if volume exists
    if ! check_volume_exists "$volume_name"; then
        log_error "Claude volume not found: $volume_name"
        log_info "Run 'agentbox' first to create the volume"
        exit 1
    fi

    case "$subcommand" in
        export|pull)
            local target_dir=""
            while [[ $# -gt 0 ]]; do
                case "$1" in
                    --to)
                        shift
                        target_dir="${1:-}"
                        if [[ -z "$target_dir" ]]; then
                            log_error "--to requires a directory path"
                            exit 1
                        fi
                        shift
                        ;;
                    *)
                        log_error "Unknown option: $1"
                        show_sync_claude_help
                        exit 1
                        ;;
                esac
            done
            pull_from_volume "$volume_name" "$target_dir"
            ;;

        -h|--help|help)
            show_sync_claude_help
            ;;

        "")
            # No subcommand - show help
            show_sync_claude_help
            ;;

        *)
            log_error "Unknown subcommand: $subcommand"
            show_sync_claude_help
            exit 1
            ;;
    esac
}

# Set up dedicated SSH directory for AgentBox
ssh_setup() {
    local agentbox_ssh="${HOME}/.agentbox/ssh"

    log_info "Setting up AgentBox SSH directory..."
    mkdir -p "${agentbox_ssh}"
    chmod 700 "${agentbox_ssh}"


    # Copy known_hosts if it exists
    if [[ -f "${HOME}/.ssh/known_hosts" ]]; then
        cp "${HOME}/.ssh/known_hosts" "${agentbox_ssh}/known_hosts"
        chmod 600 "${agentbox_ssh}/known_hosts"
        log_success "Copied known_hosts from ~/.ssh"
    fi

    # Check if AgentBox key already exists
    if [[ -f "${agentbox_ssh}/id_ed25519" ]]; then
        log_info "AgentBox SSH key already exists"
    else
        log_info "Generating dedicated SSH key for AgentBox..."
        ssh-keygen -t ed25519 -f "${agentbox_ssh}/id_ed25519" -C "agentbox@$(hostname)" -N ""
        log_success "Generated new SSH key: ${agentbox_ssh}/id_ed25519"
        echo ""
        log_info "Add this public key to your Git provider:"
        echo ""
        cat "${agentbox_ssh}/id_ed25519.pub"
        echo ""
        log_info "Alternatively, replace the keys in ${agentbox_ssh}/ with your desired keys"
    fi

    log_success "SSH setup complete! Directory: ${agentbox_ssh}"
    log_info "AgentBox will use this directory for all SSH operations."
}

# Main execution
main() {
    # Parse arguments
    local force_rebuild=false
    local cleanup_all=false
    local shell_mode=false
    local admin_mode=false
    local cmd_args=()
    declare -a ports=()
    local extra_dirs=()
    INSTANCE_NAME=""  # Global variable for instance name

    while [[ $# -gt 0 ]]; do
        case "$1" in
            -h|--help)
                show_help
                exit 0
                ;;
            -i|--instance)
                shift
                if [[ -z "${1:-}" ]]; then
                    log_error "Instance name requires a value"
                    exit 1
                fi
                # Validate instance name (alphanumeric and hyphens only)
                if [[ ! "$1" =~ ^[a-zA-Z0-9]([a-zA-Z0-9-]*[a-zA-Z0-9])?$ ]]; then
                    log_error "Instance name must be alphanumeric (hyphens allowed in middle)"
                    exit 1
                fi
                INSTANCE_NAME="$1"
                shift
                ;;
            --cleanup)
                cleanup_all=true
                shift
                ;;
            --rebuild)
                force_rebuild=true
                shift
                ;;
            -p)
                shift
                if [[ -z "${1:-}" ]]; then
                    log_error "Port argument requires a value"
                    exit 1
                fi
                ports+=("$1")
                shift
                ;;
            --add-dir)
                shift
                if [[ -z "${1:-}" ]]; then
                    log_error "--add-dir requires a directory path"
                    exit 1
                fi
                extra_dirs+=("$1")
                shift
                ;;
            list)
                check_docker
                list_instances
                exit 0
                ;;
            stop)
                check_docker
                shift
                stop_instance "${1:-}"
                exit 0
                ;;
            attach)
                check_docker
                shift
                attach_instance "${1:-}"
                exit 0
                ;;
            exec)
                check_docker
                shift
                # First arg after exec is instance name (optional)
                local exec_instance=""
                if [[ -n "${1:-}" ]] && [[ ! "$1" =~ ^- ]]; then
                    exec_instance="$1"
                    shift
                fi
                exec_instance "$exec_instance" "$@"
                exit 0
                ;;
            shell)
                shell_mode=true
                shift
                # Check if next arg is --admin
                if [[ "${1:-}" == "--admin" ]]; then
                    admin_mode=true
                    shift
                fi
                # Remaining args become shell commands
                cmd_args=("$@")
                break
                ;;
            ssh-init)
                ssh_setup
                exit 0
                ;;
            sync-claude)
                shift
                sync_claude_config "$@"
                exit 0
                ;;
            *)
                cmd_args+=("$1")
                shift
                ;;
        esac
    done

    # Check Docker
    check_docker

    # Handle special commands
    if [[ "$cleanup_all" == "true" ]]; then
        cleanup_all
        exit 0
    fi

    # Auto-assign instance name if not specified
    if [[ -z "$INSTANCE_NAME" ]]; then
        INSTANCE_NAME=$(get_next_instance_number)
        log_info "Auto-assigned instance: $INSTANCE_NAME"
    else
        log_info "Starting instance: $INSTANCE_NAME"
    fi

    # Get container name for current project
    local container_name=$(get_container_name)

    # Check if container with same name is already running
    if docker ps -q --filter "name=^${container_name}$" | grep -q .; then
        log_error "Instance already running: $container_name"
        if [[ -n "$INSTANCE_NAME" ]]; then
            log_info "Use a different instance name with -i/--instance"
        else
            log_info "Use -i/--instance to start a named instance, or 'agentbox attach' to connect"
        fi
        log_info "Use 'agentbox list' to see all running instances"
        exit 1
    fi

    # Validate dir paths (batch all errors so user sees all problems at once)
    local project_realpath
    project_realpath=$(realpath "$PROJECT_DIR")
    local validated_dirs=()
    local validation_failed=false
    for dir in "${extra_dirs[@]}"; do
        if ! validate_dir_path "$dir" validated_dirs "$project_realpath"; then
            validation_failed=true
        fi
    done

    if [[ "$validation_failed" == "true" ]]; then
        exit 1
    fi

    # Check if rebuild is needed or forced
    if [[ "$force_rebuild" == "true" ]] || needs_rebuild; then
        if [[ "$force_rebuild" == "true" ]]; then
            log_info "Forcing image rebuild..."
        else
            log_info "Dockerfile or entrypoint changed, rebuilding automatically..."
        fi

        if ! build_image; then
            log_error "Build failed!"
            exit 1
        fi

    fi

    # Run or attach to container
    if [[ "$shell_mode" == "true" ]]; then
        if [[ "$admin_mode" == "true" ]]; then
            run_container "$container_name" validated_dirs "shell" "--admin" "${cmd_args[@]}"
        else
            run_container "$container_name" validated_dirs "shell" "${cmd_args[@]}"
        fi
    else
        run_container "$container_name" validated_dirs "${cmd_args[@]}"
    fi
}

# Run main function
main "$@"
